{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD2. Vision Transformers\n",
    "By Nicolas Dufour\n",
    "\n",
    "In this TD, we will implement the Transformers architecture. Transformers has been a key architecture in deep learning for the past 5 years. \n",
    "\n",
    "It has first began with NLP, then came audio and finally, since 2020, computer vision.\n",
    "We will implement every block that makes a transformer from scratch and we will try to create a deep understanding of what is happening.\n",
    "Here is a figure for the transformer architecture:\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Miruna-Gheata/publication/355339249/figure/fig1/AS:1079476452622337@1634378650979/Encoder-decoder-architecture-of-the-Transformer-developed-by-Vaswani-et-al-28.ppm\" width=768>\n",
    "\n",
    "## Instructions\n",
    "In `pytorch` you must avoid using for loops at all cost. It's almost always possible to find a vectorized version of the operation you want to implement.\n",
    "\n",
    "**In this TP, the only for-loop you can do is the training loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install einops\n",
    "# %pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import math\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. The attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the scaled dot-product attention mechanism\n",
    "\n",
    "The transformer architecture is built around one key block: The attention.\n",
    "The idea behind attention is the following. Imagine you want to retrieve information from a dictionary. The dictionnary is indexed by keys which maps to a particular value. Now, you have a query which will be matched against the keys of the dict and if you have a match, you will retrieve the associated value.\n",
    "Attention is very similar to this simple retrieval example. Now, with real data, we don't have this structure, we however are going to learn to create it. \n",
    "\n",
    "We have 2 sets of vectors (also named tokens). One is $X_{to}$ which is the destination set. We want to be able to map this set of tokens to queries. We achieve this by doing a linear projection of $X_{to}$ to obatain:  $Q = W_QX_{to}$\n",
    "\n",
    "The other set is $X_{from}$ the set from which we want to retrieve information. We will need to extract both keys and values from this set. We therefore do 2 linear projections of $X_{from}$ to obtain:  $K = W_KX_{from}$ and $V = W_VX_{from}$.\n",
    "\n",
    "Now, contrary to the dictionnary where queries and values are exact matchs, we don't have this here. Therefore, we will perform a softer match by computing the similarity matrix between $Q$ and $K$. Then for each $Q$, we want to output the values that have the higher similarity. We therefore output the weighted sum of the values, weighted by the softmax of the similarity (also called the attention matrix).\n",
    "\n",
    "Finally, the attention operation is given by the cross attention:\n",
    "\n",
    "$$\n",
    "A(Q,K,V) = \\text{SoftMax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "We divide the similarity by $\\sqrt{d_k}$ for stability reason to avoid the similarity to explode with big vectors which would lead to very sharp attention coeficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.\n",
    "Implement the attention operation, use  `torch.einsum` to easily compute the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, x_to_dim, x_from_dim, hidden_dim,):  \n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(x_to_dim, hidden_dim)\n",
    "        self.W_K = nn.Linear(x_from_dim, hidden_dim)\n",
    "        self.W_V = nn.Linear(x_from_dim, hidden_dim)\n",
    "\n",
    "        # hidden_dim = dimension of q/k/v space\n",
    "        self.hidden_dim = hidden_dim\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x_to, x_from):\n",
    "        # x_to = [batch size, x_to_len, x_to_dim]\n",
    "        # x_from = [batch size, x_from_len, x_from_dim]\n",
    "\n",
    "        # Q = [batch size, x_to_len, hidden_dim]\n",
    "        Q = self.W_Q(x_to)\n",
    "        # K = [batch size, x_from_len, hidden_dim]\n",
    "        K = self.W_K(x_from)\n",
    "        # V = [batch size, x_from_len, hidden_dim]\n",
    "        V = self.W_V(x_from)\n",
    "\n",
    "        # QK^T = [batch size, x_to_len, x_from_len]\n",
    "        # similarity = Q @ K^T / sqrt(hidden_dim)\n",
    "        \n",
    "        similarity = torch.einsum('bqd,bkd->bqk', Q, K) / math.sqrt(self.hidden_dim)\n",
    "\n",
    "        # attention_weights\n",
    "        attention_weights = F.softmax(similarity, dim=-1)\n",
    "\n",
    "        # attention = [batch size, x_to_len, hidden_dim]\n",
    "        x_to = torch.einsum('bqk,bkd->bqd', attention_weights, V)\n",
    "\n",
    "        return x_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "We improve the above attention implementation by introducing multi-head attention. The idea here is that we compute the attention on subspaces of the $Q,K,V$ triplets. \n",
    "We split each vector in $n$ subsets and compute the attention for each subset. At the end, we concatenate every attention output and project it with an output projection.\n",
    "\n",
    "#### Question 2. \n",
    "Implement Multihead attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
    "#         # To complete\n",
    "#         super().__init__()\n",
    "\n",
    "#         assert hidden_dim % n_heads == 0\n",
    "\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.n_heads = n_heads\n",
    "#         self.head_dim = hidden_dim // n_heads\n",
    "\n",
    "#         self.W_Q = nn.Linear(x_to_dim, hidden_dim)\n",
    "#         self.W_K = nn.Linear(x_from_dim, hidden_dim)\n",
    "#         self.W_V = nn.Linear(x_from_dim, hidden_dim)\n",
    "\n",
    "#         self.W_O = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "#         # pass\n",
    "\n",
    "#     def forward(self, x_to, x_from):\n",
    "#         # x_to = [batch size, x_to_len, x_to_dim]\n",
    "#         # x_from = [batch size, x_from_len, x_from_dim]\n",
    "#         # Q, K, V = [batch size, x_len, n_heads, head_dim]\n",
    "#         Q = self.W_Q(x_to).view(batch_size, x_to_len, self.n_heads, self.head_dim)\n",
    "#         K = self.W_K(x_from).view(batch_size, x_from_len, self.n_heads, self.head_dim)\n",
    "#         V = self.W_V(x_from).view(batch_size, x_from_len, self.n_heads, self.head_dim)\n",
    "\n",
    "#         # QK^T = [batch size, n_heads, x_to_len, x_from_len]\n",
    "#         # similarity = Q @ K^T / sqrt(head_dim)\n",
    "#         similarity = torch.einsum('bqhd,bkhd->bhqk', Q, K) / math.sqrt(self.head_dim)\n",
    "\n",
    "#         attention_weights = F.softmax(similarity, dim=-1)\n",
    "#         # attention = [batch_size, n_heads, x_to_len, head_dim]\n",
    "#         x_to = torch.einsum('bhqk,bkhd->bqhd', attention_weights, V).view(batch_size, x_to_len, self.hidden_dim)\n",
    "#         # To complete\n",
    "#         return x_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(x_to_dim, hidden_dim)\n",
    "        self.W_K = nn.Linear(x_from_dim, hidden_dim)\n",
    "        self.W_V = nn.Linear(x_from_dim, hidden_dim)\n",
    "\n",
    "        self.W_O = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x_to, x_from):\n",
    "        # x_to = [batch size, x_to_len, x_to_dim]\n",
    "        # x_from = [batch size, x_from_len, x_from_dim]\n",
    "        batch_size = x_to.shape[0]\n",
    "        x_to_len = x_to.shape[1]\n",
    "        x_from_len = x_from.shape[1]\n",
    "\n",
    "        # Project and reshape\n",
    "        Q = self.W_Q(x_to).reshape(batch_size, x_to_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_K(x_from).reshape(batch_size, x_from_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_V(x_from).reshape(batch_size, x_from_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Q, K, V = [batch_size, n_heads, seq_len, head_dim]\n",
    "\n",
    "        # QK^T = [batch_size, n_heads, x_to_len, x_from_len]\n",
    "        similarity = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(similarity, dim=-1)\n",
    "\n",
    "        # attention = [batch_size, n_heads, x_to_len, head_dim]\n",
    "        attention = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape back: [batch_size, x_to_len, hidden_dim]\n",
    "        attention = attention.transpose(1, 2).contiguous().reshape(batch_size, x_to_len, self.hidden_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_O(attention)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiheadAttention is the attention that is used in transformers in pratice. It is used in 2 flavors:\n",
    "- Self Attention: When $X_{to}$ attends itself ($X_{to}=X_{from}$)\n",
    "- Cross Attention. $X_{to}\\neq X_{from}$\n",
    "\n",
    "\n",
    "#### Question 3. \n",
    "Implement MultiHead Self Attention and MultiHeadCrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def __init__(self, x_dim, hidden_dim, n_heads):\n",
    "        # self attention, x_to_dim = x_from_dim = x_dim\n",
    "        super().__init__(x_to_dim=x_dim, x_from_dim=x_dim, hidden_dim=hidden_dim, n_heads=n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, x_len, x_dim]\n",
    "        return super().forward(x_to=x, x_from=x)\n",
    "    \n",
    "class MultiHeadCrossAttention(MultiHeadAttention):\n",
    "    def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
    "        super().__init__(x_to_dim=x_to_dim, x_from_dim=x_from_dim, hidden_dim=hidden_dim, n_heads=n_heads)\n",
    "    \n",
    "    def forward(self, x_to, x_from):\n",
    "        # x_to = [batch size, x_to_len, x_to_dim]\n",
    "        # x_from = [batch size, x_from_len, x_from_dim]\n",
    "        return super().forward(x_to=x_to, x_from=x_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "Another key component of the transformer is the LayerNorm. As we have previously seen, normalizing the output of a deep learning layer helps a lot with convergence and stability. \n",
    "Until Transformers, the most used normalization is BatchNorm. We normalize the data among the batch dimension. However, this has a few problems.\n",
    "- The normalization depend on the other samples in the batch\n",
    "- When using multiple GPUs, BatchNorm needs to synchronize the batch statistic across GPUs, which locks the forward process and slow down training.\n",
    "\n",
    "The last element is the most important one. Transformers, aims to be a easy to parralilize architecture and can't afford to use batchnorm.\n",
    "\n",
    "Instead, Transformers uses Layer Norm. LayerNorm is sample dependent, which removes the synchronization issue. We normalize over the channel dimension instead of the batch dimension.\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png\">\n",
    "\n",
    "To account for the loss of capacity, we map the output by a linear transformation with a learned bias and scale.\n",
    "\n",
    "#### Question 4.\n",
    "Implement the LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [..., normalized_shape]\n",
    "        # We normalize over the last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Feedward Network\n",
    "\n",
    "Finally, the last block is a feed-forward network with one hidden layer. This layer has usually a size of $2 * input\\_dim$. This is followed by a dropout layer and an activation function. Here, we will use leaky relu, with a leak parameter of 0.1.\n",
    "\n",
    "#### Question 5.\n",
    "Implement the FFN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Sequential):\n",
    "    def __init__(self, hidden_dim, dropout_rate=0.1, expansion_factor=2):\n",
    "        super().__init__(\n",
    "            nn.Linear(hidden_dim, hidden_dim * expansion_factor),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(hidden_dim * expansion_factor, hidden_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer block\n",
    "\n",
    "The last thing that we are missing are the skip connection. Like in ResNet, the transformer architecture implements the skip-connection. This allow for a better gradient flow avoiding vanishing gradient.\n",
    "There is a skip connection after the attention and the feed forward network\n",
    "\n",
    "#### Question 6.\n",
    "Given at the transformer figure at the top, implement the Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, data_dim, hidden_dim, n_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self attention layer\n",
    "        self.self_attention = MultiHeadSelfAttention(\n",
    "            x_dim=data_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_heads=n_heads\n",
    "        )\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.norm1 = LayerNorm(normalized_shape=hidden_dim)\n",
    "        self.norm2 = LayerNorm(normalized_shape=hidden_dim)\n",
    "        \n",
    "        # Feed forward network\n",
    "        self.ffn = FFN(\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "            expansion_factor=2\n",
    "        )\n",
    "        \n",
    "        # Dropout layers for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, x_len, hidden dim]\n",
    "        \n",
    "        # First sublayer: Multi-head self attention with skip connection\n",
    "        # Norm -> Attention -> Dropout -> Skip connection\n",
    "        norm_x = self.norm1(x)\n",
    "        attention_out = self.self_attention(norm_x)\n",
    "        attention_out = self.dropout(attention_out)\n",
    "        x = x + attention_out  # Skip connection\n",
    "        \n",
    "        # Second sublayer: Feed forward network with skip connection\n",
    "        # Norm -> FFN -> Skip connection\n",
    "        norm_x = self.norm2(x)\n",
    "        ffn_out = self.ffn(norm_x)\n",
    "        x = x + ffn_out  # Skip connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embedding\n",
    "The transformers architecture is permutation independent. That means that for every token, we can swap 2 tokens and have the exact same result. However, the position of the token can be a very important information to consider. Imagine in an image. If a pixel is nearby another pixel, we want the transformer to be able to capture such information. Which is not the case for now.\n",
    "That's why we introduce positional encodings. For each token, add the positional encoding to the original token:\n",
    "\n",
    "$$\n",
    "X_i = X_i + PE(i)\n",
    "$$\n",
    "\n",
    "with X_i the token at the i dimension.\n",
    "\n",
    "The most used positional encodings are sinusoidal encodings. They are defined as follow:\n",
    "\n",
    "$$\n",
    "PE(i, 2j) = sin(i / 10000^{\\frac{2j}{d}}) \\\\\n",
    "PE(i, 2j + 1) = cos(i / 10000^{\\frac{2j}{d}})\n",
    "$$\n",
    "\n",
    "\n",
    "Where $d$ the dimension of the tokens, $i$, the i-th token in the sequence and $2j$ (resp $2j + 1$), the index of the dimension of the vector.\n",
    "The idea here is that we add a sinusoidal that encode the position in a multidimensional array.\n",
    "\n",
    "Another common positional encodings is the learned positional encoding. Simply, we let the network learn a set of tensor $PE$ that match the sequence length and dimension of the tokens.\n",
    "\n",
    "#### Question 7. \n",
    "\n",
    "Implement both Sinusoidal and Learned positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len, hidden dim]\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        # Create position indices for the sequence\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(1)  # [seq_len, 1]\n",
    "        \n",
    "        # Create dimension indices for hidden dimension pairs\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_dim, 2, device=x.device).float() * \n",
    "            (-math.log(10000.0) / hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Create empty positional encoding matrix\n",
    "        pe = torch.zeros(seq_len, hidden_dim, device=x.device)\n",
    "        \n",
    "        # Fill even indices with sin and odd indices with cos\n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "        \n",
    "        # Add positional encoding to input\n",
    "        # Expand pe to match batch dimension: [1, seq_len, hidden_dim]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create learnable parameter for positional encodings\n",
    "        self.pe = nn.Parameter(torch.zeros(1, max_len, hidden_dim))\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.xavier_uniform_(self.pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len, hidden dim]\n",
    "        \n",
    "        # Extract only the positions we need based on input sequence length\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Add positional encoding to input\n",
    "        # pe will broadcast across batch dimension\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer encoder\n",
    "Now you have everything you need to implement the transformer . You add positional encoding to the tokens and then stack N transformer encoder layers\n",
    "\n",
    "#### Question 8. \n",
    "Implement the transformer encoder with n_layers and the ability to choose both positional embeddings.\n",
    "\n",
    "Tip: Look into `ModuleList`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, data_dim, hidden_dim, n_heads, n_layers, dropout_rate=0.1, \n",
    "                 positional_encoding=\"sinusoidal\", max_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection if data_dim != hidden_dim\n",
    "        self.input_projection = (\n",
    "            nn.Linear(data_dim, hidden_dim) if data_dim != hidden_dim \n",
    "            else nn.Identity()\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        if positional_encoding == \"sinusoidal\":\n",
    "            self.positional_encoding = SinusoidalPositionalEncoding(hidden_dim)\n",
    "        elif positional_encoding == \"learned\":\n",
    "            self.positional_encoding = LearnedPositionalEncoding(hidden_dim, max_len)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown positional encoding: {positional_encoding}. \"\n",
    "                \"Choose 'sinusoidal' or 'learned'\"\n",
    "            )\n",
    "        \n",
    "        # Dropout after positional encoding\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Stack of transformer encoder blocks using ModuleList\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                data_dim=hidden_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                n_heads=n_heads,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.norm = LayerNorm(normalized_shape=hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len, data_dim]\n",
    "        \n",
    "        # Project input if necessary\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encodings\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through each transformer encoder block\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Classical Architectures: ViT & CCT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vision Transformer\n",
    "The above architecture was introduced in 2017 to process sequences of text tokens. However, it could be useful to be able to leverage this architecture for computer vision. On the contrary of convolutional neural network, the transformer has the advantage to introduce less inductive bias.\n",
    "\n",
    "This could be interesting to leverage to improve vision systems. If we learn the biases from the data, we can hope to have better performances. We however need compute and a lot of data to do this.\n",
    "\n",
    "To apply the transformer to images, one key question remains to be answered: How do we transform an image to tokens? The approach introduce in Vision Transformers is to cut the image into patches that are then transformed into a token trhought a linear projection.\n",
    "\n",
    "We also add an extra token, known as the classification token, that will be the token which will be use to predict upon. After going through the N transformer layers, this is the token that goes throught a multi layer perceptron.\n",
    "\n",
    "\n",
    "<img src= \"https://1.bp.blogspot.com/-_mnVfmzvJWc/X8gMzhZ7SkI/AAAAAAAAG24/8gW2AHEoqUQrBwOqjhYB37A7OOjNyKuNgCLcBGAsYHQ/s1600/image1.gif\" width=\"512\">\n",
    "\n",
    "\n",
    "#### Question 9\n",
    "\n",
    "Implement the vision transformer\n",
    "\n",
    "Hint: Use Conv2D with the right kernel size and stride to do the linear projection of non-overlapping patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size, hidden_dim, n_heads, n_layers, n_classes, \n",
    "                 dropout_rate=0.1, positional_encoding=\"sinusoidal\", max_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Patch embedding layer using Conv2d\n",
    "        # Input: (batch_size, 3, H, W)\n",
    "        # Output: (batch_size, n_patches, hidden_dim)\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # Learnable classification token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.transformer = TransformerEncoder(\n",
    "            data_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_heads=n_heads,\n",
    "            n_layers=n_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            positional_encoding=positional_encoding,\n",
    "            max_len=max_len\n",
    "        )\n",
    "        \n",
    "        # MLP head for classification\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),  # Common activation in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, 3, image height, image width]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Convert image to patches\n",
    "        # From [B, 3, H, W] -> [B, hidden_dim, H/patch_size, W/patch_size]\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        # Reshape to sequence of patches\n",
    "        # From [B, hidden_dim, H', W'] -> [B, H'*W', hidden_dim]\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H', W', hidden_dim]\n",
    "        x = x.reshape(batch_size, -1, self.hidden_dim)  # [B, H'*W', hidden_dim]\n",
    "        \n",
    "        # Prepend classification token\n",
    "        # Expand cls_token to match batch size\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Take cls token output and pass through MLP head\n",
    "        x = x[:, 0]  # Select only the cls token\n",
    "        x = self.mlp_head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10. \n",
    "Train a ViT on CIFAR10 for 100 epochs (for compute reason you can use only 20 epochs) and log both train and test loss and accuracy. \n",
    "We provide a data augmentation strategy called auto augment to avoid overfitting on the training data.\n",
    "Hparameters are to be choosen to your discretion.\n",
    "\n",
    "\n",
    "Tips for Hparams:\n",
    "- Don't use a transformer hidden dim too big (<256)\n",
    "- Use a small patch size\n",
    "- Use AdamW with some weight decay to avoid overfitting\n",
    "- Use between 2 and 6 transformer layers.\n",
    "- Use between 2 and 4 transformer heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_set = CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.autoaugment.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "]))\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]:   0%|          | 0/391 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     85\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     89\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# To complete: train the model, (don't forget to test it)\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "patch_size = 4  # CIFAR10 is 32x32, so 4x4 patches give us 64 patches\n",
    "hidden_dim = 192\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "n_classes = 10\n",
    "dropout_rate = 0.1\n",
    "n_epochs = 20\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = ViT(\n",
    "    patch_size=patch_size,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    n_classes=n_classes,\n",
    "    dropout_rate=dropout_rate,\n",
    "    positional_encoding=\"learned\",\n",
    "    max_len=65  # 64 patches + 1 cls token\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Training metrics storage\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Training phase\n",
    "    train_loop = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "    for images, labels in train_loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_loop.set_postfix({\n",
    "            'loss': running_loss/len(train_loader), \n",
    "            'acc': 100.*correct/total\n",
    "        })\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f'\\nEpoch {epoch+1}/{n_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and test loss\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact Convolutional Transformer\n",
    "The previous network requires a lot of compute and data to be trained. As we mentionned before, the transformer removes the inductive bias of convnets which requires more data to be trained.\n",
    "\n",
    "To bypass this, let's try out another architecture. We will try an hybrid architecture that preserves the inductive biases of convolution but manages to use the transformer to add global learning.\n",
    "\n",
    "The first change is the tokenizer. We replace it with a ConvNet. Each convnet layer has a convolution, ReLU and maxpooling.\n",
    "The second change is to actually remove the classfication token and classify on top of a pooling of all tokens. The pooling is done with an attention like mechanism:\n",
    "- For each sample, we predict a scalar, that we compute the softmax over all the sample tokens.\n",
    "- We then do an weighted average pool by this softmax values over the tokens. The weight is given by the previous step\n",
    "\n",
    "More details see: https://arxiv.org/abs/2104.05704 \n",
    "\n",
    "<img src= https://miro.medium.com/v2/resize:fit:720/format:webp/1*8diH01Fl7MhHRemLy9hUHw.png width=512>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11.\n",
    "Implement the Convolutional based tokenizer and the SeqPool operationm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPatchEmbedding(nn.Module):\n",
    "    def __init__(self, n_layers, kernel_size, hidden_dim):\n",
    "        # To complete\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, 3, image height, image width]\n",
    "        # To complete\n",
    "        pass\n",
    "\n",
    "class SeqPool(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        # To complete\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len, hidden dim]\n",
    "        # To complete\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12.\n",
    "\n",
    "Implement the Compact Convolutional Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCT(nn.Module):\n",
    "    def __init__(self, n_conv_layers, kernel_size,  n_transformer_layers, hidden_dim, n_heads, n_classes, dropout_rate=0.1):\n",
    "        # To complete\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, 3, image height, image width]\n",
    "        # To complete\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13.\n",
    "Train the CCT on CIFAR-10 for 100 epochs (for compute reason you can use only 20 epochs) and log both train and test loss and accuracy. You should obtain at least 75+% test accuracy, and observe a improvement compared to the previous ViT (Possible to get 90%+). \n",
    "We provide a data augmentation strategy called auto augment to avoid overfitting on the training data.\n",
    "Hparameters are to be choosen to your discretion.\n",
    "\n",
    "Tips for Hparams:\n",
    "- Don't use too big of a transformer hidden dim (<256)\n",
    "- For the convnet, aim to have between 32 and 128 output tokens.\n",
    "- Use AdamW with some weight decay to avoid overfitting\n",
    "- Use between 2 and 6 transformer layers.\n",
    "- Use between 2 and 4 transformer heads\n",
    "\n",
    "Training takes around 30min (depending of hparams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_set = CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.autoaugment.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "]))\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: train the model, (don't forget to test it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
